{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the decorator do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_to_class(A)\n",
    "def do(self):\n",
    "    print('Class attribute \"b\" is', self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python transforms it into:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do(self):\n",
    "    print('Class attribute \"b\" is', self.b)\n",
    "\n",
    "do = add_to_class(A)(do)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So the key question becomes:\n",
    "What does `add_to_class(A)(do)` do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expand `add_to_class`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_class(Class):\n",
    "    def wrapper(obj):\n",
    "        setattr(Class, obj.__name__, obj)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate it step-by-step\n",
    "\n",
    "First call: `add_to_class(A)`\n",
    "\n",
    "This returns the inner function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So no we effectively have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do = wrapper(do)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does `wrapper(do)` do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside `wrapper`:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(Class, obj.__name__, obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case:\n",
    "\n",
    "* `Class` → `A`\n",
    "\n",
    "* `obj` → `do`\n",
    "\n",
    "* `obj.__name__` → `\"do\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly equivalent to writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.do = do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why does this work even after instance creation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = A()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then later we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(A, 'do', do)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instances in Python don't copy methods when they're created. They look them up dynamically vi the class\n",
    "\n",
    "So when we call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.do()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python:\n",
    "\n",
    "1. Looks for `do` in `a`\n",
    "\n",
    "2. Doesn’t find it\n",
    "\n",
    "3. Looks in `A`\n",
    "\n",
    "4. Finds `do`\n",
    "\n",
    "5. Binds `self = a`\n",
    "\n",
    "6. Calls it\n",
    "\n",
    "So even existing instances gain the new method immediately.\n",
    "\n",
    "This is pure Python dynamic behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progress Board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the args?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They’re plot styling / axis configuration defaults for Matplotlib.\n",
    "\n",
    "* `xscale='linear'`, `yscale='linear'`\n",
    "\n",
    "    * Controls the axis scale.\n",
    "\n",
    "    * `'linear'` means normal spacing.\n",
    "\n",
    "    * Common alternatives: `'log'`, `'symlog'`, `'logit'`.\n",
    "\n",
    "    * In the code, they get applied here:\n",
    "\n",
    "        ```\n",
    "        axes.set_xscale(self.xscale)\n",
    "        axes.set_yscale(self.yscale)\n",
    "        ```\n",
    "\n",
    "* `ls=['-', '--', '-.', ':']`\n",
    "\n",
    "    * A list of linestyles (solid, dashed, dash-dot, dotted).\n",
    "\n",
    "    * Used so different series (different labels) are visually distinguishable:\n",
    "\n",
    "        ```\n",
    "        for (k, v), ls, color in zip(self.data.items(), self.ls, self.colors):\n",
    "            d2l.plt.plot(..., linestyle=ls, ...)\n",
    "\n",
    "        ```\n",
    "\n",
    "Important gotcha: because they use `zip(...)`, you can only display as many labels as the shortest list length. With the defaults, you’ll see up to 4 lines (unless you extend `ls` and `colors`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why does `ProgressBoard` inherit from `d2l.HyperParameters`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `HyperParameters` provides the method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and `ProgressBoard.__init__` calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.save_hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So inheritance is used to reuse that utility: automatically turn all constructor arguments into attributes, without writing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.xlabel = xlabel\n",
    "self.ylabel = ylabel\n",
    "self.xlim = xlim\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plot()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `\\` Backslash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = self.trainer.train_batch_idx / \\\n",
    "    self.trainer.num_train_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \\ means:\n",
    "\n",
    "* Continue this line on the next line.\n",
    "\n",
    "So Python reads this as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = self.trainer.train_batch_idx / self.trainer.num_train_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is `x`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = self.trainer.train_batch_idx / self.trainer.num_train_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose:\n",
    "* 100 batches per epoch\n",
    "* current batch = 30\n",
    "\n",
    "Then:\n",
    "\n",
    "x = 30 / 100 = 0.3\n",
    "\n",
    "So during training, x moves smoothly from 0 -> 1\n",
    "\n",
    "This makes the plot update continuously within an epoch\n",
    "\n",
    "Validation case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = self.trainer.epoch + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If current epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation is plotted once per epoch, so it jumps discretely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is `n`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = self.trainer.num_train_batches / self.plot_train_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If:\n",
    "\n",
    "* 100 batches per epoch\n",
    "* `plot_train_per_epoch = 2`\n",
    "\n",
    "Then:\n",
    "\n",
    "n = 100 / 2 =50\n",
    "\n",
    "Meaning:\n",
    "* Only plot every 50 batches\n",
    "\n",
    "This controls smoothing frequency\n",
    "\n",
    "Later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "every_n = int(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so `draw()` will only plot once every `n` cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Most Interesting Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.board.draw(\n",
    "    x,\n",
    "    value.to(d2l.cpu()).detach().numpy(),\n",
    "    ('train_' if train else 'val_') + key,\n",
    "    every_n=int(n)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "('train_' if train else 'val_') + key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"loss\"\n",
    "train = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"train_loss\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"val_loss\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So training and validation losses appear as separate lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Step and Validation Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference between `training_step` and `validation_step`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They do the same forward + loss computation, but differ in what happens next:\n",
    "\n",
    "* training_step\n",
    "\n",
    "    * Computes loss l\n",
    "\n",
    "    * Calls plot(..., train=True) so it’s logged as \"train_loss\"\n",
    "\n",
    "    * Returns l (so the Trainer can call backward() and optimizer.step())\n",
    "\n",
    "* validation_step\n",
    "\n",
    "    * Computes loss l\n",
    "\n",
    "    * Calls plot(..., train=False) so it’s logged as \"val_loss\"\n",
    "\n",
    "    * Does not return anything (validation typically has no backprop / no optimizer step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does `self.loss(self(*batch[:-1]), batch[-1])` mean?\n",
    "\n",
    "Assume each `batch` is like `(X, y)`.\n",
    "\n",
    "* `batch[:-1]` = everything except the last element → `(X,)`\n",
    "\n",
    "* `*batch[:-1]` splats into positional args → `self(X)`\n",
    "\n",
    "* `self(X)` calls `Module.__call__` (from `nn.Module`), which internally calls `forward(X)` → produces `y_hat`\n",
    "\n",
    "* `batch[-1]` = last element → `y`\n",
    "\n",
    "* `self.loss(y_hat, y)` computes the loss tensor `l`\n",
    "\n",
    "So it’s a compact way of supporting batches like `(X, y)` or even `(X1, X2, ..., y)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-by-step example: 2 epochs, 10 batches each\n",
    "\n",
    "Let’s assume:\n",
    "\n",
    "* `num_train_batches = 10`\n",
    "\n",
    "* `num_val_batches = 10` (just for illustration)\n",
    "\n",
    "* `plot_train_per_epoch = 2` (default)\n",
    "\n",
    "* `plot_valid_per_epoch = 1` (default)\n",
    "\n",
    "* The trainer maintains:\n",
    "\n",
    "    * `trainer.epoch` (0-based)\n",
    "\n",
    "    * `trainer.train_batch_idx` (batch index within current epoch, typically 0..9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 0 (first epoch)\n",
    "Training loop, batches 0..9\n",
    "\n",
    "Each batch does:\n",
    "\n",
    "1. `training_step(batch)`:\n",
    "\n",
    "    * `y_hat = self(X) (forward)`\n",
    "\n",
    "    * `l = loss(y_hat, y)`\n",
    "\n",
    "    * `plot('loss', l, train=True)`\n",
    "\n",
    "2. Inside `plot(..., train=True)`:\n",
    "\n",
    "    * Check `self.trainer` exists\n",
    "\n",
    "    * Compute:\n",
    "\n",
    "        * x = train_batch_idx / 10\n",
    "\n",
    "        * every_n = 5\n",
    "\n",
    "        * label = \"train_loss\"\n",
    "\n",
    "    * Call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board.draw(x, loss_value, \"train_loss\", every_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation loop, batches 0..9 (after training epoch 0)\n",
    "\n",
    "For each val batch:\n",
    "\n",
    "1. `validation_step(batch)`:\n",
    "\n",
    "    * compute `l`\n",
    "\n",
    "    * `plot('loss', l, train=False)`\n",
    "\n",
    "2. Inside `plot(..., train=False)`:\n",
    "\n",
    "    * `x = epoch + 1 = 1`\n",
    "\n",
    "    * `every_n = 10`\n",
    "\n",
    "    * label = `\"val_loss\"`\n",
    "\n",
    "    * call `board.draw(1, loss_value, \"val_loss\", every_n=10)`\n",
    "\n",
    "3. Inside `board.draw(... every_n=10)`:\n",
    "\n",
    "    * collects 10 raw points\n",
    "\n",
    "    * only draws when it has all 10\n",
    "\n",
    "So:\n",
    "\n",
    "* Val batch 0..8: not drawn yet\n",
    "\n",
    "* Val batch 9: the 10th point → draw happens\n",
    "\n",
    "    * x-values are all 1\n",
    "\n",
    "So validation loss is plotted once per epoch (at x=epoch+1)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
